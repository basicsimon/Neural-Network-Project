# -*- coding: utf-8 -*-
"""WNN_AnswersExercise05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gp4rEp3eduBp1s4MbBHThVc9yTzy-0gr

Working with Neural Network Models

Â© Hans Nieminen, Satakunta University of Applied Sciences

# Exercise 5.1
"""

import numpy as np
from sklearn.preprocessing import OneHotEncoder

def cross_entropy(y, y_pred):
    """Parameter ``y`` contains the target labels as
    one-hot encoded values, e.g. np.array([[1,0,0], [0,0,1], [0,1,0]])
    Parameter ``y_hat`` contains the predictions as probability spaces,
    e.g. np.array([[0.8,0.1,0.1],[0.2,0.1,0.7],[0.3,0.4,0.3]])"""
    loss = 0

    # Doing cross entropy Loss
    for i in range(len(y_pred)):
      loss -= y[i].dot(np.log(y_pred[i]))

    return loss/len(y_pred)

y = np.array([[0],
              [2],
              [2],
              [1],
              [0]])
y_pred = np.array([[0.6, 0.15, 0.25],
                   [0.1, 0.2, 0.7],
                   [0.2, 0.35, 0.45],
                   [0.1, 0.5, 0.4],
                   [0.5, 0.2, 0.3]])

y_ohe = OneHotEncoder().fit_transform(y).toarray()
y_ohe

loss_value = cross_entropy(y_ohe, y_pred)
print(loss_value.round(3))

y_pred2 = np.array([[0.8, 0.15, 0.05],
                    [0.1, 0.15, 0.75],
                    [0.1, 0.15, 0.75],
                    [0.1, 0.7, 0.2],
                    [0.6, 0.2, 0.2]])

loss_value2 = cross_entropy(y_ohe, y_pred2)
print(loss_value2.round(3))

print('decrease of the loss value in percents',
      round((loss_value-loss_value2)/loss_value,2)*100)

"""# Exercise 5.2"""

import numpy as np

class ANN(object):
    def __init__(self, sizes, hidden_act = "none", output_act = "none",
                 random_state = None):
        """The list ``sizes`` contains the number of neurons in the
        respective layers of the network.
        For example, if the list was [2, 3, 1] then it would be a
        three-layer network, with the first layer containing 2 neurons,
        the second layer 3 neurons, and the third layer 1 neuron.
        The biases and weights for the network are initialized randomly,
        using a Gaussian distribution with mean 0, and variance 1.
        Note that the first layer is assumed to be an input layer.
        Parameter ``hidden_act`` contains the activation function name used
        in hidden layers.
        Parameter ``output_act`` contains the activation function name used
        in output layer."""
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.input_size = sizes[0]
        self.output_size = sizes[-1]

        # biases and weights together
        self.W = []
        rng = np.random.RandomState(random_state)
        for i in np.arange(0, self.num_layers - 2):
            # initialize weight matrix (the + 1 is for the bias)
            w = rng.randn(self.sizes[i] + 1, self.sizes[i + 1] )
            self.W.append(w / np.sqrt(self.sizes[i]))
        # last layer is different: it doesn't need a bias
        w = rng.randn(self.sizes[-2] + 1, self.sizes[-1])
        self.W.append(w / np.sqrt(self.sizes[-2]))

        # set activation functions for hidden layers
        if hidden_act.lower() == "sigmoid":
            self.hidden_act = self.sigmoid
        elif hidden_act.lower() == "tanh":
            self.hidden_act = self.tanh
        elif hidden_act.lower() == "relu":
            self.hidden_act = self.ReLU
        elif hidden_act.lower() == "none":
            self.hidden_act = self.no_activation
        else:
            raise ValueError('Parameter hidden_act has to be one of these: "sigmoid", "tanh", "ReLU", "none".')

        # set activation function for output layer
        if output_act.lower() == 'none':
            self.output_act = self.no_activation
        elif output_act.lower() == "sigmoid":
            self.output_act = self.sigmoid
        elif output_act.lower() == "softmax":
            self.output_act = self.softmax
        else:
           raise ValueError('Parameter output_act has to be one of these: "sigmoid", "softmax", "none".')

    def predict(self, X, addBias = True):
        p = np.atleast_2d(X)

        if addBias:
            p = np.c_[p, np.ones((p.shape[0]))]

        # loop over layers
        for layer in np.arange(0, len(self.W)):
            if layer == len(self.W)-1:
                p = self.output_act(np.dot(p, self.W[layer]))
            else:
                p = self.hidden_act(np.dot(p, self.W[layer]))
                if addBias:
                    p = np.c_[p, np.ones((p.shape[0]))]
        return p

    # Activation functions
    def sigmoid(self, x):
        return 1/(1+np.exp(-x))

    def tanh(self, x):
        return np.tanh(x)

    def ReLU(self, x):
        return (x > 0) * x

    def no_activation(self, x):
        return x

    def softmax(self, x):
        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

ann = ANN(sizes=[12,10,8,5],
           hidden_act = "ReLU",
           output_act = "softmax",
           random_state = 123)

ann.W

X = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.1, 0.2, 0.3, 0.4])

y_pred = ann.predict(X)
y_pred

print('Predicted value is', y_pred.argmax(axis=1))