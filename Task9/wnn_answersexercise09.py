# -*- coding: utf-8 -*-
"""WNN_AnswersExercise09.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fgrn7bnoLTFSooTQlcVoq7P9tevLVeGz

Working with Neural Network Models

Â© Hans Nieminen, Satakunta University of Applied Sciences

# Exercise 9.1
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import numpy as np
from itertools import product

device = "cuda" if torch.cuda.is_available() else "cpu"
device

"""## Data"""

# Load the Wine Quality dataset
data = pd.read_csv("https://raw.githubusercontent.com/haniemi/deeplearning/main/data/winequality_red.csv", delimiter=';')

data.head()

data["quality"].value_counts()

# Separate features and target
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

X[:5]

# Standardize the X dataset
scaler = StandardScaler()
X = scaler.fit_transform(X)

y[:5]

# Encode labels to integers starting from 0
encoder = LabelEncoder()
y = encoder.fit_transform(y)

y[:5]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=8)

# Convert to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

class CustomDataset(Dataset):
    def __init__(self, X, y, device='cpu'):
      self.X = X.to(device)
      self.y = y.to(device)

    def __len__(self):
      return len(self.y)

    def __getitem__(self, idx):
      return self.X[idx], self.y[idx]

# Create a custom dataset
train_dataset = CustomDataset(X_train, y_train, device)

# Create a dataloader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)

"""## Neural network"""

# Define the neural network
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(11,64)
        nn.init.kaiming_normal_(self.layer1.weight,
                                nonlinearity='relu')
        nn.init.zeros_(self.layer1.bias)
        self.layer1_act = nn.ReLU()
        self.layer2 = nn.Linear(64, 32)
        nn.init.kaiming_normal_(self.layer2.weight,
                                nonlinearity='relu')
        nn.init.zeros_(self.layer2.bias)
        self.layer2_act = nn.ReLU()
        self.layer3 = nn.Linear(32, 10)

    def forward(self, x):
        x = self.layer1_act(self.layer1(x))
        x = self.layer2_act(self.layer2(x))
        x = self.layer3(x)
        return x

# Create the model
torch.manual_seed(99)
model = NeuralNetwork().to(device)

# Train the model
num_epochs = 30

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

"""### Training"""

# Training loop
model.train()
for epoch in range(num_epochs):
    running_loss = 0.0
    for inputs, targets in train_loader:
        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, targets)

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)

    epoch_loss = running_loss / len(train_loader.dataset)
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')

"""### Evaluating"""

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    outputs = model(X_test.to(device))
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y_test.to(device)).sum().item() / y_test.size(0)
    print(f'Test Accuracy: {accuracy:.4f}')

"""### Hyperparameter tuning"""

# Hyperparameter tuning
learning_rates = [0.0005, 0.001, 0.01, 0.1]
betas = [(0.9, 0.999), (0.95, 0.999), (0.99, 0.999), (0.9, 0.990), (0.9, 0.985), (0.9, 0.98)]

best_accuracy = 0
best_params = None

for lr, beta in product(learning_rates, betas):
    torch.manual_seed(99)
    model = NeuralNetwork().to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr, betas=beta)

    # Training loop
    model.train()
    for epoch in range(num_epochs):
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

    # Evaluate the model
    model.eval()
    with torch.no_grad():
        outputs = model(X_test.to(device))
        _, predicted = torch.max(outputs, 1)
        accuracy = (predicted == y_test.to(device)).sum().item() / y_test.size(0)
        print(f'Learning Rate: {lr}, Betas: {beta}, Test Accuracy: {accuracy:.4f}')

        # Check if we have a new best accuracy
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = (lr, beta)

print(f'Best Hyperparameters - Learning Rate: {best_params[0]}, Betas: {best_params[1]}, Accuracy: {best_accuracy:.4f}')