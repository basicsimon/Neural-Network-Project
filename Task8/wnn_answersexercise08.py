# -*- coding: utf-8 -*-
"""WNN_AnswersExercise08.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Aj1SXZHaQ76w5Vu7JztACNM3fCZk9L9

Working with Neural Network Models

Â© Hans Nieminen, Satakunta University of Applied Sciences

# Exercise 8.1
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np

device = "cuda" if torch.cuda.is_available() else "cpu"
device

# Load the California Housing dataset
california = fetch_california_housing()
X, y = california.data, california.target

# Standardize the X data
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=827)

train_size = len(X_train)
train_size

# Convert to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

X_train[:3]

y_train[:3]

class CustomDataset(Dataset):
    def __init__(self, X, y, device='cpu'):
      self.X = X.to(device)
      self.y = y.to(device)

    def __len__(self):
      return len(self.y)

    def __getitem__(self, idx):
      return self.X[idx], self.y[idx]

# Create a custom dataset
train_dataset = CustomDataset(X_train, y_train, device)

# Create a dataloader
train_loader = DataLoader(train_dataset,
                          batch_size=64,
                          shuffle=False)  # = no shuffling

# Define the neural network
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(8,16)
        nn.init.kaiming_normal_(self.layer1.weight,
                                nonlinearity='relu')
        nn.init.zeros_(self.layer1.bias)
        self.layer1_act = nn.ReLU()
        self.layer2 = nn.Linear(16, 8)
        nn.init.kaiming_normal_(self.layer2.weight,
                                nonlinearity='relu')
        nn.init.zeros_(self.layer2.bias)
        self.layer2_act = nn.ReLU()
        self.layer3 = nn.Linear(8, 1)

    def forward(self, x):
        x = self.layer1_act(self.layer1(x))
        x = self.layer2_act(self.layer2(x))
        x = self.layer3(x)
        return x

# Create the model
torch.manual_seed(41)
model = NeuralNetwork().to(device)

for name, parameter in model.named_parameters():
  print(name)
  print(parameter)
  print()

"""Question 1: What is the maximum value for the weights in the second hidden layer? Give the answer rounded to two decimals."""

l2_max_weight = model.layer2.weight.cpu().detach().numpy().max()

l2_max_weight

print(f'The second hidden layer has the weight maximum of {l2_max_weight:.2f}')

"""Question 2: What is the Loss value after the 10th epoch? Give the answer rounded to three decimals."""

# Set the loss and optimizer
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(),
                      lr=0.015,
                      momentum=0.9)

# Train the model
num_epochs = 10
model.train()
for epoch in range(num_epochs):
    running_loss = 0.0
    for batch_X, batch_y in train_loader:
        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        # The loss.item() gives the mean loss for a batch
        # So we multiply it with batch size to get total loss of all samples in a batch
        running_loss += loss.item() * batch_X.size(0)

    #epoch_loss = running_loss / len(train_loader.dataset)
    epoch_loss = running_loss / train_size
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')

"""Question 3: What is the value of the test loss? Give the answer rounded to three decimals."""

# Evaluate the model on the test set
model.eval()
with torch.inference_mode():
#with torch.no_grad():
    predictions = model(X_test.to(device))
    test_loss = criterion(predictions, y_test.to(device))
    print(f'Test Loss: {test_loss.item():.3f}')

"""Solution with no initialization of biases."""

# Define the neural network (no intialization of biases)
class NeuralNetworkV2(nn.Module):
    def __init__(self):
        super(NeuralNetworkV2, self).__init__()
        self.layer1 = nn.Linear(8,16)
        nn.init.kaiming_normal_(self.layer1.weight,
                                nonlinearity='relu')
        self.layer1_act = nn.ReLU()
        self.layer2 = nn.Linear(16, 8)
        nn.init.kaiming_normal_(self.layer2.weight,
                                nonlinearity='relu')
        self.layer2_act = nn.ReLU()
        self.layer3 = nn.Linear(8, 1)

    def forward(self, x):
        x = self.layer1_act(self.layer1(x))
        x = self.layer2_act(self.layer2(x))
        x = self.layer3(x)
        return x

# Create the model
torch.manual_seed(41)
model2 = NeuralNetworkV2().to(device)

# Set the loss and optimizer
criterion2 = nn.MSELoss()
optimizer2 = optim.SGD(model2.parameters(),
                      lr=0.015,
                      momentum=0.9)

# Train the model
num_epochs = 10
for epoch in range(num_epochs):
    model2.train()
    running_loss = 0.0
    for batch_X, batch_y in train_loader:
        # Zero the parameter gradients
        optimizer2.zero_grad()

        # Forward pass
        outputs = model2(batch_X)
        loss = criterion2(outputs, batch_y)

        # Backward pass and optimize
        loss.backward()
        optimizer2.step()

        # The loss.item() gives the mean loss for a batch
        # So we multiply it with batch size to get total loss of all samples in a batch
        running_loss += loss.item() * batch_X.size(0)

    #epoch_loss = running_loss / len(train_loader.dataset)
    epoch_loss = running_loss / train_size
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')

# Evaluate the model on the test set
model2.eval()
with torch.inference_mode():
#with torch.no_grad():
    predictions = model2(X_test.to(device))
    test_loss = criterion2(predictions, y_test.to(device))
    print(f'Test Loss: {test_loss.item():.3f}')